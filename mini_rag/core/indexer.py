import os, json
from pathlib import Path
from typing import Tuple
from dotenv import load_dotenv; load_dotenv()
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from core.drive_auth import load_creds_auto
from core.drive_io import build_drive, list_pdfs, download_pdfs_to_docs

ROOT = Path(__file__).resolve().parents[1]
OUTDIR = ROOT / "data" / "raw"
INDEX_DIR = ROOT / "index"
CRED = ROOT / "credentials.json"
TOKEN = ROOT / "token.json"
SCOPES = ["https://www.googleapis.com/auth/drive.readonly"]

def _check_gpu_availability() -> bool:
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False

def _load_emb():
    """GPUê°€ ìˆìœ¼ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” OpenAI Embeddingsë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    gpu_available = _check_gpu_availability()
    
    if gpu_available:
        print("ğŸš€ GPU ê°ì§€ë¨ - GPU ê°€ì†ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        return OpenAIEmbeddings(
            model="text-embedding-3-small",  # GPUì—ì„œ ë” íš¨ìœ¨ì ì¸ ëª¨ë¸
            dimensions=1536
        )
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤")
        return OpenAIEmbeddings()

def _index_paths():
    idx = INDEX_DIR / "faiss_idx"; meta = INDEX_DIR / "faiss_meta.json"
    idx.mkdir(parents=True, exist_ok=True)
    return idx, meta

def build_or_load_vectorstore(drive_folder: str = "", shared_folder: str = "") -> FAISS:
    emb = _load_emb()
    idx, meta = _index_paths()
    
    # í´ë” ID ì¡°í•©ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"{drive_folder}_{shared_folder}"
    
    if (idx / "index.faiss").exists() and meta.exists():
        m = json.loads(meta.read_text(encoding="utf-8"))
        if m.get("cache_key") == cache_key:
            return FAISS.load_local(str(idx), emb, allow_dangerous_deserialization=True)
    return rebuild_vectorstore(drive_folder, shared_folder)

def rebuild_vectorstore(drive_folder: str = "", shared_folder: str = "") -> FAISS:
    print(f"ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ì¬êµ¬ì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"   ğŸ“ Drive í´ë”: {drive_folder or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
    print(f"   ğŸ“ Shared í´ë”: {shared_folder or 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
    
    # í´ë” ID ê²€ì¦
    if not drive_folder and not shared_folder:
        raise RuntimeError("Google Drive í´ë” IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì—ì„œ DRIVE_FOLDER ë˜ëŠ” SHARED_FOLDERë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    
    try:
        creds = load_creds_auto(CRED, SCOPES, TOKEN)
        drive = build_drive(creds)
        print("âœ… Google Drive API ì—°ê²° ì„±ê³µ")
        
        all_files = []
        all_docs = []
        
        # Drive í´ë” ì²˜ë¦¬
        if drive_folder:
            files, _ = list_pdfs(drive, drive_folder, "Drive")
            if files:
                all_files.extend(files)
                docs = download_pdfs_to_docs(drive, files, OUTDIR)
                all_docs.extend(docs)
        
        # Shared í´ë” ì²˜ë¦¬
        if shared_folder:
            files, _ = list_pdfs(drive, shared_folder, "Shared")
            if files:
                all_files.extend(files)
                docs = download_pdfs_to_docs(drive, files, OUTDIR)
                all_docs.extend(docs)
        
        if not all_files:
            raise RuntimeError("ì„¤ì •ëœ í´ë”ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ë” IDë¥¼ í™•ì¸í•˜ê±°ë‚˜ í´ë”ì— PDF íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        
        if not all_docs:
            raise RuntimeError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        print(f"ğŸ“ {len(all_docs)}ê°œì˜ ë¬¸ì„œë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤...")
        splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)
        chunks = [c for c in splitter.split_documents(all_docs) if c.page_content.strip()]
        
        if not chunks: 
            raise RuntimeError("ì²­í¬ 0ê±´ â†’ PDFì— í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ OCRì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        print(f"ğŸ“„ {len(chunks)}ê°œì˜ ì²­í¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤...")
        emb = _load_emb()
        vs = FAISS.from_documents(chunks, emb)
        
        idx, meta = _index_paths()
        vs.save_local(str(idx))
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"{drive_folder}_{shared_folder}"
        meta.write_text(json.dumps({
            "cache_key": cache_key,
            "drive_folder": drive_folder,
            "shared_folder": shared_folder,
            "doc_count": len(chunks)
        }), encoding="utf-8")
        
        print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {len(chunks)}ê°œ ì²­í¬")
        return vs
        
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
        raise
