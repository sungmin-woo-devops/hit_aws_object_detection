#!/usr/bin/env python3
"""
ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ - ë²„í‚· prefix ì „ëµ ë° ë¡œì»¬ ë””ë ‰í„°ë¦¬ ê´€ë¦¬

ì´ ëª¨ë“ˆì€ AI ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì˜ ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ì„ ê´€ë¦¬í•©ë‹ˆë‹¤:
- ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
- ì „ì²˜ë¦¬ ë° ë¼ë²¨ë§ ì¤€ë¹„
- ë°ì´í„°ì…‹ êµ¬ì„± ë° ë²„ì „ ê´€ë¦¬
- ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ ì €ì¥
"""

import os
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import yaml
from tqdm import tqdm

import modules.minio as minio


class DataManager:
    """ë°ì´í„° ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, bucket_name: str, local_root: str = "./data"):
        self.bucket_name = bucket_name
        self.local_root = Path(local_root)
        
        # ë²„í‚· prefix ì •ì˜
        self.prefixes = {
            'raw': 'raw/',
            'processed': 'processed/',
            'datasets': 'datasets/',
            'models': 'models/',
            'exports': 'exports/'
        }
        
        # ë¡œì»¬ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ì •ì˜
        self.local_dirs = {
            'raw': self.local_root / 'raw',
            'processed': self.local_root / 'processed',
            'datasets': self.local_root / 'datasets',
            'models': self.local_root / 'models',
            'temp': self.local_root / 'temp'
        }
        
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ë¡œì»¬ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„±"""
        for dir_path in self.local_dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def upload_raw_data(self, local_path: str, data_type: str = 'images') -> bool:
        """ì›ë³¸ ë°ì´í„° ì—…ë¡œë“œ"""
        local_file = Path(local_path)
        if not local_file.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {local_path}")
            return False
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        key = f"{self.prefixes['raw']}{data_type}/{timestamp}_{local_file.name}"
        
        return minio.upload_file(self.bucket_name, str(local_file), key, quiet=False)
    
    def upload_processed_data(self, local_path: str, status: str = 'unlabeled') -> bool:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì—…ë¡œë“œ"""
        local_file = Path(local_path)
        if not local_file.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {local_path}")
            return False
        
        key = f"{self.prefixes['processed']}{status}/{local_file.name}"
        return minio.upload_file(self.bucket_name, str(local_file), key, quiet=False)
    
    def create_dataset_version(self, dataset_name: str, version: str = None) -> str:
        """ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë²„ì „ ìƒì„±"""
        if version is None:
            version = datetime.now().strftime("v%Y%m%d_%H%M%S")
        
        dataset_prefix = f"{self.prefixes['datasets']}{dataset_name}_{version}/"
        
        # ë°ì´í„°ì…‹ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„± (MinIOì—ì„œëŠ” ë¹ˆ í´ë”ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±)
        metadata = {
            'dataset_name': dataset_name,
            'version': version,
            'created_at': datetime.now().isoformat(),
            'status': 'created'
        }
        
        # ë¡œì»¬ì— ì„ì‹œ ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„± í›„ ì—…ë¡œë“œ
        temp_file = self.local_dirs['temp'] / f"{dataset_name}_{version}_metadata.yaml"
        with open(temp_file, 'w', encoding='utf-8') as f:
            yaml.dump(metadata, f, allow_unicode=True)
        
        metadata_key = f"{dataset_prefix}metadata.yaml"
        minio.upload_file(self.bucket_name, str(temp_file), metadata_key, quiet=False)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        temp_file.unlink()
        
        print(f"âœ… ë°ì´í„°ì…‹ '{dataset_name}_{version}' ìƒì„± ì™„ë£Œ")
        return dataset_prefix
    
    def upload_dataset(self, 
                      local_dataset_path: str, 
                      dataset_name: str, 
                      version: str = None,
                      include_labels: bool = True) -> str:
        """ì™„ì„±ëœ ë°ì´í„°ì…‹ ì—…ë¡œë“œ"""
        dataset_path = Path(local_dataset_path)
        if not dataset_path.exists():
            print(f"âŒ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {local_dataset_path}")
            return ""
        
        dataset_prefix = self.create_dataset_version(dataset_name, version)
        
        # ì´ë¯¸ì§€ ë° ë¼ë²¨ íŒŒì¼ ì—…ë¡œë“œ
        uploaded_count = 0
        failed_count = 0
        
        for split in ['train', 'test', 'val']:
            split_path = dataset_path / split
            if not split_path.exists():
                continue
            
            # ì´ë¯¸ì§€ ì—…ë¡œë“œ
            images_dir = split_path / 'images'
            if images_dir.exists():
                for img_file in images_dir.glob('*'):
                    if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                        key = f"{dataset_prefix}{split}/images/{img_file.name}"
                        if minio.upload_file(self.bucket_name, str(img_file), key, quiet=True):
                            uploaded_count += 1
                        else:
                            failed_count += 1
            
            # ë¼ë²¨ ì—…ë¡œë“œ (ì„ íƒì )
            if include_labels:
                labels_dir = split_path / 'labels'
                if labels_dir.exists():
                    for label_file in labels_dir.glob('*'):
                        if label_file.suffix.lower() in ['.txt', '.xml', '.json', '.npy']:
                            key = f"{dataset_prefix}{split}/labels/{label_file.name}"
                            if minio.upload_file(self.bucket_name, str(label_file), key, quiet=True):
                                uploaded_count += 1
                            else:
                                failed_count += 1
        
        # data.yaml íŒŒì¼ ì—…ë¡œë“œ
        data_yaml = dataset_path / 'data.yaml'
        if data_yaml.exists():
            key = f"{dataset_prefix}data.yaml"
            if minio.upload_file(self.bucket_name, str(data_yaml), key, quiet=False):
                uploaded_count += 1
            else:
                failed_count += 1
        
        print(f"ğŸ“¦ ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        return dataset_prefix
    
    def download_dataset(self, 
                        dataset_prefix: str, 
                        local_path: str = None) -> str:
        """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        if local_path is None:
            local_path = self.local_dirs['datasets'] / dataset_prefix.replace('/', '_').rstrip('_')
        
        local_path = Path(local_path)
        local_path.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„°ì…‹ íŒŒì¼ë“¤ ëª©ë¡ ì¡°íšŒ
        dataset_files = minio.list_files_by_prefix(self.bucket_name, dataset_prefix)
        
        downloaded_count = 0
        for file_info in tqdm(dataset_files, desc="ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘"):
            key = file_info['Key']
            relative_path = key[len(dataset_prefix):]
            local_file_path = local_path / relative_path
            
            # ë””ë ‰í„°ë¦¬ ìƒì„±
            local_file_path.parent.mkdir(parents=True, exist_ok=True)
            
            if minio.download_file(self.bucket_name, key, str(local_file_path), quiet=True):
                downloaded_count += 1
            else:
                print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {key}")
        
        print(f"âœ… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_count}ê°œ íŒŒì¼")
        return str(local_path)
    
    def list_datasets(self) -> List[Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ"""
        datasets = []
        dataset_files = minio.list_files_by_prefix(self.bucket_name, self.prefixes['datasets'])
        
        # ë°ì´í„°ì…‹ë³„ë¡œ ê·¸ë£¹í™”
        dataset_groups = {}
        for file_info in dataset_files:
            key = file_info['Key']
            parts = key.replace(self.prefixes['datasets'], '').split('/')
            if len(parts) >= 1:
                dataset_id = parts[0]
                if dataset_id not in dataset_groups:
                    dataset_groups[dataset_id] = []
                dataset_groups[dataset_id].append(file_info)
        
        for dataset_id, files in dataset_groups.items():
            dataset_info = {
                'id': dataset_id,
                'file_count': len(files),
                'last_modified': max(f['LastModified'] for f in files),
                'prefix': f"{self.prefixes['datasets']}{dataset_id}/"
            }
            datasets.append(dataset_info)
        
        return sorted(datasets, key=lambda x: x['last_modified'], reverse=True)
    
    def migrate_existing_data(self):
        """ê¸°ì¡´ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        print("ğŸ”„ ê¸°ì¡´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        # 1. AWS-Icon-Detector--4 ë°ì´í„°ì…‹ ë§ˆì´ê·¸ë ˆì´ì…˜
        aws_dataset_path = "./AWS-Icon-Detector--4"
        if Path(aws_dataset_path).exists():
            print("ğŸ“¦ AWS Icon Detector v4 ë°ì´í„°ì…‹ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
            self.upload_dataset(aws_dataset_path, "aws_icon_detector", "v4")
        
        # 2. processed í´ë” ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
        processed_path = self.local_dirs['processed']
        if processed_path.exists():
            print("ğŸ”§ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
            for img_file in processed_path.glob('*.png'):
                self.upload_processed_data(str(img_file), 'unlabeled')
        
        print("âœ… ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
    
    def generate_local_config(self, dataset_prefix: str) -> str:
        """ë¡œì»¬ í•™ìŠµìš© data.yaml ì„¤ì • íŒŒì¼ ìƒì„±"""
        # ë°ì´í„°ì…‹ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ
        local_dataset_path = self.download_dataset(dataset_prefix)
        
        # data.yaml íŒŒì¼ ê²½ë¡œ ìˆ˜ì •
        data_yaml_path = Path(local_dataset_path) / 'data.yaml'
        if data_yaml_path.exists():
            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # ê²½ë¡œë¥¼ ë¡œì»¬ ê²½ë¡œë¡œ ìˆ˜ì •
            base_path = Path(local_dataset_path)
            config['train'] = str(base_path / 'train' / 'images')
            config['val'] = str(base_path / 'test' / 'images')  # validationìœ¼ë¡œ test ì‚¬ìš©
            config['test'] = str(base_path / 'test' / 'images')
            
            # ìˆ˜ì •ëœ ì„¤ì • ì €ì¥
            with open(data_yaml_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, allow_unicode=True)
            
            print(f"âœ… ë¡œì»¬ ì„¤ì • íŒŒì¼ ìƒì„±: {data_yaml_path}")
            return str(data_yaml_path)
        
        return ""


def show_data_structure_guide():
    """ë°ì´í„° êµ¬ì¡° ê°€ì´ë“œ ì¶œë ¥"""
    print("""
ğŸ—‚ï¸  ë°ì´í„° ê´€ë¦¬ êµ¬ì¡° ê°€ì´ë“œ

ğŸ“ ë¡œì»¬ ë””ë ‰í„°ë¦¬:
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # ì›ë³¸ ë°ì´í„° (SVG, ì´ë¯¸ì§€)
â”‚   â”œâ”€â”€ processed/              # ì „ì²˜ë¦¬ëœ ë°ì´í„° (ë¼ë²¨ë§ ëŒ€ê¸°)
â”‚   â”œâ”€â”€ datasets/               # ì™„ì„±ëœ ë°ì´í„°ì…‹ë“¤
â”‚   â”œâ”€â”€ models/                 # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
â”‚   â””â”€â”€ temp/                   # ì„ì‹œ ì‘ì—… íŒŒì¼

â˜ï¸  MinIO ë²„í‚· êµ¬ì¡°:
â”œâ”€â”€ raw/                        # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ svg_files/              # SVG ì›ë³¸ë“¤
â”‚   â””â”€â”€ images/                 # ê¸°íƒ€ ì´ë¯¸ì§€ ì›ë³¸ë“¤
â”œâ”€â”€ processed/                  # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â”œâ”€â”€ unlabeled/              # ë¼ë²¨ë§ ëŒ€ê¸° ì¤‘
â”‚   â””â”€â”€ temp/                   # ì„ì‹œ ì²˜ë¦¬ íŒŒì¼ë“¤
â”œâ”€â”€ datasets/                   # ì™„ì„±ëœ ë°ì´í„°ì…‹ë“¤
â”‚   â”œâ”€â”€ aws_icon_detector_v4/   # AWS Icon Detector v4
â”‚   â””â”€â”€ custom_dataset_v1/      # ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹
â”œâ”€â”€ models/                     # í•™ìŠµëœ ëª¨ë¸ë“¤
â”‚   â”œâ”€â”€ weights/                # ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â””â”€â”€ experiments/            # ì‹¤í—˜ ê²°ê³¼
â””â”€â”€ exports/                    # ë°°í¬ìš© ëª¨ë¸

ğŸ”„ ë°ì´í„° íë¦„:
raw â†’ processed â†’ datasets â†’ models â†’ exports

ğŸ“ ì‚¬ìš© ì˜ˆì‹œ:
```python
# ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”
dm = DataManager('aws-diagram-object-detection')

# ê¸°ì¡´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
dm.migrate_existing_data()

# ìƒˆ ë°ì´í„°ì…‹ ì—…ë¡œë“œ
dm.upload_dataset('./my-dataset', 'custom_icons', 'v1')

# ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ
datasets = dm.list_datasets()

# í•™ìŠµìš© ë¡œì»¬ ì„¤ì • ìƒì„±
config_path = dm.generate_local_config('datasets/aws_icon_detector_v4/')
```
""")


if __name__ == "__main__":
    show_data_structure_guide()